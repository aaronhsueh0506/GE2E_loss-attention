{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "create on Wed Sep 16 13:14:29 2020\n",
    "\n",
    "@author: Mingyu Hsueh\n",
    "\n",
    "Environment:\n",
    "    Tensorflow 2.0\n",
    "    Python 3.8 ++\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from tensorflow import keras\n",
    "import import_ipynb\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, LambdaCallback\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, Input, Layer, Reshape, Lambda, GRU, Bidirectional,\\\n",
    "                                    Flatten, BatchNormalization, TimeDistributed, \\\n",
    "                                    ReLU, GlobalAveragePooling1D, Concatenate, Dense\n",
    "from utils import normalize, regression, Ge2e_loss, test_similarity, EER_estimate, Ge2eOptimizer \n",
    "# , Centroid_matrix, Centroid_matrix_basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1 import ConfigProto, InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "InteractiveSession(config=config)\n",
    "\n",
    "tf.compat.v1.disable_eager_execution() # disable eager_excution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, state_size, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        self.hidden = state_size   \n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Trainable parameters\n",
    "        initializer = tf.random_normal_initializer(stddev=0.1)\n",
    "        self.w_omega = self.add_weight(name=\"w_omega\", shape=[self.hidden, 1024], \n",
    "                                       initializer=initializer)\n",
    "        self.b_omega = self.add_weight(name=\"b_omega\", shape=[1024], initializer=initializer)\n",
    "        self.u_omega = self.add_weight(name=\"u_omega\", shape=[1024], initializer=initializer)\n",
    "        super(Attention, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        v = tf.tanh(tf.tensordot(inputs, self.w_omega, axes=1) + self.b_omega)\n",
    "\n",
    "        # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
    "        vu = tf.tensordot(v, self.u_omega, axes=1, name='vu')  # (B,T) shape\n",
    "        alphas = tf.nn.softmax(vu, name='alphas')         # (B,T) shape\n",
    "\n",
    "        # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n",
    "        context = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n",
    "        return context\n",
    "    \n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         shape = tf.TensorShape(input_shape).as_list()\n",
    "#         shape[-1] = self.output_dim\n",
    "#         return tf.TensorShape(shape)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'output_dim':self.hidden}\n",
    "        base_config = super(Attention, self).get_config()\n",
    "        return dict(list(base_config.items())+list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(Model):\n",
    "    def __init__(self, config, *args, **kwargs):\n",
    "        super(Network, self).__init__(*args, **kwargs)\n",
    "        self.mode = False\n",
    "        \n",
    "        self.n_layer = config[\"n_layer\"]\n",
    "        self.rnn_unit = config[\"hidden_size\"]\n",
    "        self.lr = config[\"lr\"]\n",
    "        self.dense_unit = config[\"proj_size\"]\n",
    "        self.M = config[\"m_file\"]\n",
    "        self.N = config[\"batch_size\"]\n",
    "        self.timestamps = config[\"time_stamp\"]\n",
    "        self.feature_dim = config[\"feature_dim\"]\n",
    "        self.loss_type = config[\"loss_type\"]\n",
    "        self.steps = config[\"steps\"]\n",
    "        self.gen_batch_size = config[\"batch_size\"]\n",
    "        self.total_utt = config[\"n_train\"]\n",
    "        self.epoch = config[\"epoch\"]\n",
    "        self.checkpoint_path = config[\"checkpoint_path\"]\n",
    "        \n",
    "        self.train_path = config[\"train_path\"]\n",
    "        self.enroll_path = config[\"enroll_path\"]\n",
    "        self.eval_path = config[\"eval_path\"]\n",
    "        self.ground_path = config[\"ground_path\"]\n",
    "        \n",
    "        self.inputs=[]\n",
    "        self.outputs=[]\n",
    "        self.infer_input=[]\n",
    "        # layer       \n",
    "        self.conv1 = Conv2D(filters=16, kernel_size=[5,5], padding='same', name='conv1')\n",
    "        self.bn1 = BatchNormalization()\n",
    "        self.relu1 = ReLU()\n",
    "        \n",
    "        self.conv2 = Conv2D(filters=16, kernel_size=[5,5], padding='same', name='conv2')\n",
    "        self.bn2 = BatchNormalization()\n",
    "        self.relu2 = ReLU()\n",
    "        \n",
    "        self.conv3 = Conv2D(filters=32, kernel_size=[5,5], padding='same', name='conv3')\n",
    "        self.bn3 = BatchNormalization()\n",
    "        self.relu3 = ReLU()\n",
    "        \n",
    "        self.conv4 = Conv2D(filters=32, kernel_size=[5,5], padding='same', name='conv4')\n",
    "        self.bn4 = BatchNormalization()\n",
    "        self.relu4 = ReLU()\n",
    "        \n",
    "        self.flatten = TimeDistributed(Flatten())\n",
    "        # input layers\n",
    "        self.input_layer = Input(shape=(None, self.timestamps, self.feature_dim), name='input')\n",
    "        self.squeeze_layer = Lambda(lambda x: tf.keras.backend.squeeze(x, axis=0), name='squeeze_input')\n",
    "        self.expand_layer = Lambda(lambda x: tf.expand_dims(x, axis=-1), name='expand_input')\n",
    "        \n",
    "        self.lstm_layers = []\n",
    "        self.dense_layers = []\n",
    "        for lstm_idx in range(self.n_layer):\n",
    "            return_sequences = True\n",
    "            lstm_layer = Bidirectional(GRU(units=(self.rnn_unit), return_sequences=return_sequences, name=\"lstm_{}\".format(lstm_idx)))\n",
    "            dense_layer = TimeDistributed(Dense(units=(2*self.dense_unit), name=\"dense_{}\".format(lstm_idx)))            \n",
    "                \n",
    "            self.lstm_layers.append(lstm_layer)\n",
    "            self.dense_layers.append(dense_layer)\n",
    "        self.embedding_norm_layer = tf.keras.layers.Lambda(lambda x: tf.keras.backend.l2_normalize(x, axis=-1), \n",
    "                                                            name='embeding_output')\n",
    "        \n",
    "        self.attention = Attention(2*self.dense_unit)\n",
    "#         self._centroid_layer = Centroid_matrix(self.N, self.M,name='centroids')\n",
    "#         self.infer_centroid_layer = Centroid_matrix_basic(self.N, self.M,name='infer_centroids')\n",
    "\n",
    "        self.regression = regression(self.N, self.M, 2*self.dense_unit, True, name='similarity_regression') \n",
    "        self.infer_regression = regression(self.N, self.M, 2*self.dense_unit, False, name='similarity_regression2') \n",
    "        self.loss_layer = Ge2e_loss(self.N, self.M, self.loss_type, name='ge2e_loss')\n",
    "        \n",
    "        self.X = None    \n",
    "\n",
    "    def _build(self, is_training=False):\n",
    "        self.X = self.input_layer\n",
    "        self.inputs.append(self.X)\n",
    "        self.X = self.squeeze_layer(self.X)\n",
    "        self.X = self.expand_layer(self.X)\n",
    "        \n",
    "        self.X = self.conv1(self.X)\n",
    "        self.X = self.relu1(self.bn1(self.X))\n",
    "        \n",
    "        self.X = self.conv2(self.X)\n",
    "        self.X = self.relu2(self.bn2(self.X))\n",
    "        \n",
    "        self.X = self.conv3(self.X)\n",
    "        self.X = self.relu3(self.bn3(self.X))\n",
    "        \n",
    "        self.X = self.conv4(self.X)\n",
    "        self.X = self.relu4(self.bn4(self.X))\n",
    "        \n",
    "        self.X = self.flatten(self.X)\n",
    "        \n",
    "        # multi lstm + dense\n",
    "        for lstm_idx in range(self.n_layer):\n",
    "            self.X = self.lstm_layers[lstm_idx](self.X)\n",
    "            self.X = self.dense_layers[lstm_idx](self.X)\n",
    "        self.X = self.attention(self.X)\n",
    "        self.X = self.embedding_norm_layer(self.X)       # [tot_utt, embed_dim]\n",
    "        \n",
    "        # loss layer\n",
    "#         self._centroid = self._centroid_layer(self.X)\n",
    "#         self.matrix = self.regression((self.X,self._centroid))\n",
    "        self.matrix = self.regression(self.X)\n",
    "        self.loss = self.loss_layer(self.matrix)\n",
    "        \n",
    "        # outputs\n",
    "        self.outputs.append(self.loss)\n",
    "        self.outputs.append(self.X)\n",
    "\n",
    "        model = Model(inputs=self.inputs, outputs=self.outputs)                  \n",
    "        return model\n",
    "    \n",
    "    def generator(self):\n",
    "        train_data = np.load(self.train_path)\n",
    "        \n",
    "        batch_x = np.empty((0,99,40))\n",
    "        train_index = [i for i in range(self.total_utt)]\n",
    "        random.shuffle(train_index)\n",
    "\n",
    "        loss = np.zeros((1,))\n",
    "\n",
    "        while True:\n",
    "            for i in range(self.steps):\n",
    "                batch_list = [train_index.pop() for _ in range(min(self.gen_batch_size, len(train_index)))]\n",
    "                for idx in batch_list:\n",
    "                    batch_x = np.concatenate((batch_x, train_data[idx]), axis=0)\n",
    "\n",
    "                output = np.zeros((1,batch_x.shape[0], 64))\n",
    "                batch_x = np.expand_dims(batch_x, axis=0)\n",
    "#                 batch_x = np.expand_dims(batch_x, axis=-1)\n",
    "                yield ({'input': batch_x}, {'ge2e_loss': loss, 'embeding_output': output})\n",
    "                batch_x = np.empty((0,99,40))\n",
    "            train_index = [i for i in range(self.total_utt)]\n",
    "            random.shuffle(train_index)\n",
    "        \n",
    "    def _run(self):\n",
    "        self.model = self._build()\n",
    "        self.model.compile(optimizer=Ge2eOptimizer(self.lr), loss=[custom_loss, dummy_loss])\n",
    "        self.model.summary()\n",
    "        \n",
    "        callbacks = self._callbacks()\n",
    "#         print('steps_per_epoch',self.steps)\n",
    "        if not os.path.isdir(self.checkpoint_path):\n",
    "            os.makedirs(self.checkpoint_path)\n",
    "        self.model.fit(self.generator(), steps_per_epoch=self.steps, epochs=self.epoch, verbose=1, \\\n",
    "                            use_multiprocessing=True, callbacks=callbacks, shuffle=False)\n",
    "        return \n",
    "    \n",
    "    def _callbacks(self):\n",
    "        logdir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard_callback = TensorBoard(log_dir=logdir, write_graph=True)\n",
    "        epoch_print_callback = LambdaCallback(on_epoch_end=lambda epochs,logs: self.val())\n",
    "        cp_save_callback = ModelCheckpoint(os.path.join(self.checkpoint_path, \"model_{epoch:02d}.ckpt\"), \\\n",
    "                                           save_weights_only=True, verbose=0, save_freq=5*self.steps)\n",
    "        \n",
    "        callbacks = [cp_save_callback, tensorboard_callback, epoch_print_callback]\n",
    "        return callbacks\n",
    "    \n",
    "    def _infer_build(self):\n",
    "        # inputs\n",
    "        self.X = self.input_layer\n",
    "        self.infer_input=[]\n",
    "        self.infer_input.append(self.X)\n",
    "        self.X = self.squeeze_layer(self.X)\n",
    "        self.X = self.expand_layer(self.X)\n",
    "\n",
    "        self.X = self.conv1(self.X)\n",
    "        self.X = self.relu1(self.bn1(self.X))\n",
    "        \n",
    "        self.X = self.conv2(self.X)\n",
    "        self.X = self.relu2(self.bn2(self.X))\n",
    "        \n",
    "        self.X = self.conv3(self.X)\n",
    "        self.X = self.relu3(self.bn3(self.X))\n",
    "        \n",
    "        self.X = self.conv4(self.X)\n",
    "        self.X = self.relu4(self.bn4(self.X))\n",
    "        \n",
    "        self.X = self.flatten(self.X)\n",
    "        \n",
    "        # multi lstm + dense\n",
    "        for lstm_idx in range(self.n_layer):\n",
    "            self.X = self.lstm_layers[lstm_idx](self.X)\n",
    "            self.X = self.dense_layers[lstm_idx](self.X)\n",
    "        self.X = self.attention(self.X)\n",
    "        self.X = self.embedding_norm_layer(self.X)       # [tot_utt, embed_dim]\n",
    "        \n",
    "        # loss layer\n",
    "#         self._centroid = self.infer_centroid_layer(self.X)\n",
    "        self.matrix = self.infer_regression((self.X,self._centroid))\n",
    "\n",
    "        self.loss = self.loss_layer(self.matrix)\n",
    "        \n",
    "        model = tf.keras.Model(inputs=[self.infer_input], outputs=[self.X, self.loss])\n",
    "        return model\n",
    "    \n",
    "    def _infer_process(self, index):\n",
    "        model = self._infer_build()\n",
    "        if index<10: model_name = \"model_0\" + str(index) + \".ckpt\"\n",
    "        else: model_name = \"model_\"+ str(index) + \".ckpt\"\n",
    "\n",
    "        new_model = tf.keras.Model(model.inputs, model.layers[-3].output)\n",
    "#         print(new_model.get_weights())\n",
    "        new_model.load_weights(os.path.join(self.checkpoint_path, model_name), by_name=True, skip_mismatch=True)\n",
    "#         print(new_model.get_weights())\n",
    "        \n",
    "        if not self.mode: \n",
    "            new_model.summary()\n",
    "            self.mode = True\n",
    "        \n",
    "        un_en_data = np.load(self.enroll_path)  \n",
    "        en_data = np.expand_dims(un_en_data, axis=0)\n",
    "        ev_data = np.load(self.eval_path)\n",
    "        ev_data = self.data_linein(ev_data)\n",
    "        \n",
    "        enroll_stack = np.empty((0,self.dense_unit))\n",
    "        eval_stack = np.empty((0,self.dense_unit))\n",
    "        \n",
    "        for i in range(en_data.shape[1]):\n",
    "            _enroll= np.empty((0,self.dense_unit))\n",
    "            for j in range(en_data.shape[2]):\n",
    "                data = np.reshape(en_data[:,i,j,:,:],(1,1,self.timestamps, self.feature_dim))\n",
    "                enroll = new_model.predict(data, batch_size=1)\n",
    "                _enroll = np.concatenate((_enroll, enroll), axis=0)\n",
    "            enroll_stack = np.concatenate((enroll_stack,np.mean(_enroll,0,keepdims=True)),axis=0)\n",
    "        print(enroll_stack.shape)\n",
    "        \n",
    "        for i in range(ev_data.shape[1]):\n",
    "            data = np.reshape(ev_data[:,i,:,:,:],(1,1,self.timestamps, self.feature_dim))\n",
    "            evalu = new_model.predict(data, batch_size=1)\n",
    "            eval_stack = np.concatenate((eval_stack, evalu), axis=0)\n",
    "        print(eval_stack.shape)\n",
    "        \n",
    "        similarity = test_similarity(enroll_stack, eval_stack)\n",
    "        print(similarity)\n",
    "        ground = np.load(self.ground_path)\n",
    "        EER = EER_estimate(similarity, ground, draw=True)\n",
    "        return EER\n",
    "    \n",
    "    def data_linein(self, data):\n",
    "        if len(data.shape)==4:\n",
    "            speakers = len(data)\n",
    "            batch = np.empty((0,99,40))\n",
    "            for i in range(speakers):\n",
    "                batch = np.concatenate((batch, data[i]), axis=0)\n",
    "            batch = np.expand_dims(batch, axis=0)\n",
    "#             batch = np.expand_dims(batch, axis=-1)\n",
    "        elif len(data.shape)==3:\n",
    "            batch = np.expand_dims(data, axis=0)\n",
    "#             batch = np.expand_dims(batch, axis=-1)\n",
    "        else: pass\n",
    "        return batch\n",
    "    \n",
    "    def train_test(self, index):\n",
    "        model = self._infer_build()\n",
    "        if index<10: model_name = \"model_0\" + str(index) + \".ckpt\"\n",
    "        else: model_name = \"model_\"+ str(index) + \".ckpt\"\n",
    "            \n",
    "#         model.load_weights(os.path.join(self.checkpoint_path, model_name), by_name=True, skip_mismatch=True)\n",
    "        new_model = tf.keras.Model(model.inputs, model.layers[-3].output)\n",
    "        new_model.summary()\n",
    "#         print(os.path.join(self.checkpoint_path, model_name))\n",
    "#         print(new_model.get_weights())\n",
    "        new_model.load_weights(os.path.join(self.checkpoint_path, model_name), by_name=True, skip_mismatch=True)\n",
    "#         print(new_model.get_weights())\n",
    "        \n",
    "        _data = np.load(self.train_path)\n",
    "        index=[random.randint(0,_data.shape[0]) for _ in range(5)]\n",
    "        data = np.empty((0,_data.shape[1],_data.shape[2],_data.shape[3]))\n",
    "        \n",
    "        for i in index:\n",
    "            data = np.concatenate((data,np.reshape(_data[i],(1,_data.shape[1],_data.shape[2],_data.shape[3]))),axis=0)\n",
    "        stack = np.empty((0,data.shape[1],self.dense_unit))\n",
    "        \n",
    "        for i in range(data.shape[0]):\n",
    "            tmp= np.empty((0,self.dense_unit))\n",
    "            for j in range(data.shape[1]):\n",
    "                batch = np.reshape(data[i,j,:,:],(1,1,self.timestamps, self.feature_dim,1))\n",
    "                _tmp = new_model.predict(batch, batch_size=1)\n",
    "                tmp = np.concatenate((tmp, _tmp), axis=0)\n",
    "            stack = np.concatenate((stack,np.reshape(tmp,(1,tmp.shape[0],tmp.shape[1]))),axis=0)\n",
    "        \n",
    "        similarity = test_similarity(np.mean(stack,1), stack)\n",
    "        print(np.round(similarity,3))\n",
    "    \n",
    "    def val(self):\n",
    "        new_model = tf.keras.Model(self.model.inputs, self.model.layers[-3].output)\n",
    "        \n",
    "        _data = np.load(self.train_path)\n",
    "        index=[random.randint(0,_data.shape[0]) for _ in range(3)]\n",
    "        data = np.empty((0,_data.shape[1],_data.shape[2],_data.shape[3]))\n",
    "        \n",
    "        for i in index:\n",
    "            data = np.concatenate((data,np.reshape(_data[i],(1,_data.shape[1],_data.shape[2],_data.shape[3]))),axis=0)\n",
    "        stack = np.empty((0,data.shape[1],self.dense_unit))\n",
    "        \n",
    "        for i in range(data.shape[0]):\n",
    "            tmp= np.empty((0,self.dense_unit))\n",
    "            for j in range(data.shape[1]):\n",
    "                batch = np.reshape(data[i,j,:,:],(1,1,self.timestamps, self.feature_dim,1))\n",
    "                _tmp = new_model.predict(batch, batch_size=1)\n",
    "                tmp = np.concatenate((tmp, _tmp), axis=0)\n",
    "            stack = np.concatenate((stack,np.reshape(tmp,(1,tmp.shape[0],tmp.shape[1]))),axis=0)\n",
    "        \n",
    "        similarity = test_similarity(np.mean(stack,1), stack)\n",
    "        print(np.round(similarity,3))\n",
    "        \n",
    "    \n",
    "#     def compute_output_shape(self, input_shape):\n",
    "#         assert(input_shape, list)\n",
    "#         shape_a, shape_b = input_shape\n",
    "#         return [(shape_a[0], self.output_dim), shape_b[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom loss\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return y_pred\n",
    "\n",
    "def dummy_loss(y_true, y_pred):\n",
    "    return 0 * y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "# parser = argparse.ArgumentParser()\n",
    "# args = parser.parse_args()\n",
    "\n",
    "CONFIG_FILE = os.getcwd() +'/'+ 'config.json'\n",
    "with open(CONFIG_FILE, 'rb') as fid:\n",
    "    config = json.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"n_train\"] = 1418\n",
    "config[\"m_file\"] = 4\n",
    "config[\"time_stamp\"] = 99\n",
    "config[\"feature_dim\"] = 40 \n",
    "# config[\"n_train\"], config[\"m_file\"], config[\"time_stamp\"], config[\"feature_dim\"] = train_data.shape\n",
    "\n",
    "config[\"steps\"] = math.floor(config[\"n_train\"] / config[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1635: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           [(None, None, 99, 40)]    0         \n",
      "_________________________________________________________________\n",
      "squeeze_input (Lambda)       (None, 99, 40)            0         \n",
      "_________________________________________________________________\n",
      "expand_input (Lambda)        (None, 99, 40, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 99, 40, 16)        416       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 99, 40, 16)        64        \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 99, 40, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 99, 40, 16)        6416      \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 99, 40, 16)        64        \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 99, 40, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (None, 99, 40, 32)        12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 99, 40, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 99, 40, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (None, 99, 40, 32)        25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 99, 40, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu_3 (ReLU)               (None, 99, 40, 32)        0         \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 99, 1280)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 99, 256)           1082880   \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 99, 128)           32896     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 99, 256)           198144    \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 99, 128)           32896     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 99, 256)           198144    \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 99, 128)           32896     \n",
      "_________________________________________________________________\n",
      "attention (Attention)        (None, 128)               133120    \n",
      "_________________________________________________________________\n",
      "embeding_output (Lambda)     (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "similarity_regression (regre (64, 16)                  2         \n",
      "_________________________________________________________________\n",
      "ge2e_loss (Ge2e_loss)        (1,)                      0         \n",
      "=================================================================\n",
      "Total params: 1,756,658\n",
      "Trainable params: 1,756,466\n",
      "Non-trainable params: 192\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.\n",
      "Epoch 1/2500\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[99,64,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node bidirectional/forward_lstm_0_1/TensorArrayV2Stack/TensorListStack}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[loss/Identity_1/_221]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[99,64,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node bidirectional/forward_lstm_0_1/TensorArrayV2Stack/TensorListStack}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c6114591a77e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(hist.history)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-cb7fda4a7a33>\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m                             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1121\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3566\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3567\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3568\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3569\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1472\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1473\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1474\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[99,64,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node bidirectional/forward_lstm_0_1/TensorArrayV2Stack/TensorListStack}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[loss/Identity_1/_221]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[99,64,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node bidirectional/forward_lstm_0_1/TensorArrayV2Stack/TensorListStack}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "sv = Network(config)\n",
    "sv._run()\n",
    "# print(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "EER_dict = {}\n",
    "sv = Network(config)\n",
    "num_of_model = len(glob.glob('/home4/myhsueh/save_model/*.ckpt'))\n",
    "for idx in range(num_of_model,0,-1):\n",
    "    EER_val = sv._infer_process((idx)*2)\n",
    "    EER_dict[idx*2] = EER_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
