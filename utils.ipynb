{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "create on Wed Sep 16 09:29:29 2020\n",
    "\n",
    "@author: Mingyu Hsueh\n",
    "\n",
    "Environment:\n",
    "    Tensorflow 2.0\n",
    "    Python 3.8 ++\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.python.ops import nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Optimizer\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.python.keras.optimizer_v2.gradient_descent import SGD\n",
    "class Ge2eOptimizer(SGD):\n",
    "    \"\"\"\n",
    "    Note:\n",
    "        Inherited from tf.keras.optimizer.SGD\n",
    "    Attributes:\n",
    "        __init__: constructs Ge2eOptimizer class\n",
    "        get_updates: compute and modify gradients\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "             learning_rate,\n",
    "             momentum=0.0,\n",
    "             nesterov=False,\n",
    "             **kwargs):\n",
    "        \"\"\"\n",
    "        Note:\n",
    "        Args:\n",
    "            learning_rate: SGD parameter\n",
    "            momentum: SGD parameter\n",
    "            nesterov: SGD parameter\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "\n",
    "        super(Ge2eOptimizer, self).__init__(**kwargs)\n",
    "\n",
    "    def get_updates(self, loss, params):\n",
    "        \"\"\"\n",
    "        Note:\n",
    "            Compute gradients and modify them according to the GE2E paper\n",
    "        Args:\n",
    "            loss: loss to be minimized\n",
    "            params: all trainable variables in the model\n",
    "        Returns:\n",
    "            A list of modified gradients to be applied\n",
    "        \"\"\"\n",
    "        \n",
    "        grads = self.get_gradients(loss, params)\n",
    "#         print(grads)\n",
    "        grads_clip, _ = tf.clip_by_global_norm(grads, 3.0)\n",
    "        grads_rescale= grads_clip[:-2] + [0.01*grad for grad in grads_clip[-2:]]   # 0.01 for w,b in similarity\n",
    "        grads_and_vars = list(zip(grads_rescale, params))\n",
    "        \n",
    "        return [self.apply_gradients(grads_and_vars)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "        l2 normalize for input vector x\n",
    "        return: normalized input\n",
    "    \"\"\"\n",
    "    return nn.l2_normalize(x,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # my regression layer does not work\n",
    "\n",
    "class regression(Layer):\n",
    "    def __init__(self, N, M, dense_unit, flag, **kwargs):\n",
    "        super(regression, self).__init__(**kwargs)\n",
    "        \n",
    "        self.N = N\n",
    "        self.M = M\n",
    "        self.dense_unit = dense_unit\n",
    "                \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(name=\"w\", shape=[1], dtype=tf.float32, \n",
    "#                                  initializer=tf.keras.initializers.Constant(10.), trainable=True)\n",
    "                                 initializer=tf.constant_initializer(value=10.), trainable=True, \\\n",
    "                                 constraint=tf.keras.constraints.NonNeg())\n",
    "        self.b = self.add_weight(name=\"b\", shape=[1], dtype=tf.float32, \n",
    "                                 initializer=tf.keras.initializers.Constant(-5.), trainable=True)\n",
    "\n",
    "        super(regression, self).build(input_shape)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'N':self.N, 'M':self.M, 'dense_unit':self.dense_unit}\n",
    "        base_config = super(regression, self).get_config()\n",
    "        return dict(list(base_config.items())+list(config.items()))\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Calculate similarity matrix\n",
    "        input: embedding (NM * P)\n",
    "        return: similarity matrix in tf type (NM * N)\n",
    "        \"\"\"\n",
    "        # reshape to [N, M, P]\n",
    "        embedded_split = tf.reshape(inputs, shape=[self.N, self.M, self.dense_unit])\n",
    "        center = normalize(tf.reduce_mean(embedded_split, axis=1))          # [N,P] normalized center vectors eq.(1)\n",
    "        center_except = normalize(tf.reshape((tf.reduce_sum(embedded_split, axis=1, keepdims=True) \\\n",
    "                        - embedded_split), shape=[self.N*self.M, self.dense_unit]))  # [NM,P] center vectors eq.(8)\n",
    "        # make similarity matrix eq.(9)\n",
    "        S = tf.concat(\n",
    "            [tf.concat([tf.reduce_sum(center_except[i*self.M:(i+1)*self.M,:]*embedded_split[j,:,:], axis=1, \\\n",
    "                                      keepdims=True) if i==j\n",
    "                        else tf.reduce_sum(center[i:(i+1),:]*embedded_split[j,:,:], axis=1, keepdims=True) \\\n",
    "                        for i in range(self.N)], axis=1) for j in range(self.N)], axis=0)\n",
    "        \n",
    "        w_S = tf.abs(self.w)*S + self.b\n",
    "\n",
    "        return w_S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class regression(Layer):\n",
    "#     def __init__(self, N, M, dense_unit, flag, **kwargs):\n",
    "#         super(regression, self).__init__(**kwargs)\n",
    "        \n",
    "#         self.N = N\n",
    "#         self.M = M\n",
    "#         self.dense_unit = dense_unit\n",
    "#         self.flag = flag\n",
    "                \n",
    "#     def build(self, input_shape):\n",
    "#         self.w = self.add_weight(name=\"w\", initializer=keras.initializers.Constant(value=10),trainable=True)\n",
    "#         self.b = self.add_weight(name=\"b\", initializer=keras.initializers.Constant(value=-5),trainable=True)\n",
    "#         super(regression, self).build(input_shape)\n",
    "        \n",
    "#     def call(self, inputs):\n",
    "#         if self.flag:\n",
    "#             # [tot_utt, embed_dim]\n",
    "#             utterances = inputs[0]\n",
    "#             # [tot_utt, embed_dim, num_spkr]\n",
    "#             centroids = tf.keras.backend.permute_dimensions(inputs[1], [1, 0, 2])\n",
    "\n",
    "#             l2_utterances = nn.l2_normalize(utterances, axis=1)\n",
    "#             l2_centroids = nn.l2_normalize(centroids, axis=1)\n",
    "\n",
    "#             similarity = K.batch_dot(l2_utterances, l2_centroids, axes=[1, 1])\n",
    "#         else:\n",
    "#             l2_utterances = tf.nn.l2_normalize(inputs[0], axis=-1)\n",
    "#             l2_centroids = tf.nn.l2_normalize(K.transpose(inputs[1]), axis=0)\n",
    "#             similarity = K.dot(l2_utterances, l2_centroids)\n",
    "            \n",
    "#         self.weight = tf.clip_by_value(self.w, 1e-6, np.infty)\n",
    "#         weighted_similarity = self.w * similarity + self.b\n",
    "#         return weighted_similarity\n",
    "    \n",
    "#     def get_config(self):\n",
    "#         config = {'N':self.N, 'M':self.M, 'output_dim': self.dense_unit}\n",
    "#         base_config = super(regression, self).get_config()\n",
    "#         return dict(list(base_config.items())+list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Centroid_matrix(Layer):\n",
    "#     def __init__(self, num_speakers, num_utterance, **kwargs):\n",
    "#         super(Centroid_matrix, self).__init__(**kwargs)\n",
    "#         self.N = num_speakers\n",
    "#         self.M = num_utterance\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         super(Centroid_matrix, self).build(input_shape)\n",
    "\n",
    "#     def call(self, inputs):\n",
    "\n",
    "#         # input shape [tot_utt, embed_dim]\n",
    "#         inputs = tf.keras.backend.permute_dimensions(inputs, [1, 0])  # [embed_dim, tot_utt]\n",
    "        \n",
    "#         # centroid_column\n",
    "#         self_block = tf.keras.backend.ones(shape=[self.M, self.M], dtype=tf.float32) - \\\n",
    "#                                             tf.keras.backend.eye(self.M, dtype=tf.float32)\n",
    "#         self_block = self_block / (self.M - 1) # subtract itself and mean\n",
    "        \n",
    "#         # [num_spkr_utt, num_spkr_utt]\n",
    "#         centroid_block = tf.pad(self_block, [[0, 0], [0, (self.N - 1) * self.M]], name=\"normal_centroid_select_pad\", \\\n",
    "#                                 constant_values=1/self.M) # other speakers mean\n",
    "        \n",
    "#         # [num_spkr_utt * num_spkr, num_spkr_utt]\n",
    "#         centroid_per_spkr = tf.pad(centroid_block, [[0, (self.N - 1) * self.M], [0, 0]], name=\"other_utterances_zero\", \\\n",
    "#                                    constant_values=0)\n",
    "#         # [tot_utt, tot_utt]\n",
    "\n",
    "#         # [tot_utt, tot_utt]\n",
    "#         centroid_per_spkr_list = [tf.roll(centroid_per_spkr, axis=0, shift=spk_idx * self.M) for spk_idx in range(self.N)]\n",
    "#         # num_spkr * [tot_utt, tot_utt]\n",
    "#         centroid_list = tf.keras.backend.stack(centroid_per_spkr_list, axis=-1)\n",
    "#         # [tot_utt, tot_utt, num_spkr]\n",
    "\n",
    "#         self_exclusive_centroids = tf.keras.backend.dot(inputs, centroid_list)\n",
    "#         # [embed_dim, tot_utt] * [tot_utt, tot_utt, num_spkr]\n",
    "#         # ---> [embed_dim, tot_utt, num_spkr]\n",
    "#         return self_exclusive_centroids\n",
    "    \n",
    "#     def get_config(self):\n",
    "#         config = {'Nspeakers':self.N, 'Mutterance':self.M}\n",
    "#         base_config = super(Centroid_matrix, self).get_config()\n",
    "#         return dict(list(base_config.items())+list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Centroid_matrix_basic(Layer):\n",
    "#     \"\"\"\n",
    "#     Note:\n",
    "#         Compute centroids of all speakers (including the utterance itself)\n",
    "#     Attributes:\n",
    "#         __init__:\n",
    "#         build: Creates the variables of the layer\n",
    "#         call: Compute centroid matrix of speaker embeddings\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, num_speakers, num_utterance, **kwargs):\n",
    "#         \"\"\"\n",
    "#         Note:\n",
    "#         Args:\n",
    "#             num_speakers: number of speakers (in the paper, it is 64)\n",
    "#             num_utterance: number of utterances per speaker (in the paper, it is 10)\n",
    "#         Returns:\n",
    "#         \"\"\"\n",
    "\n",
    "#         super(Centroid_matrix_basic, self).__init__(**kwargs)\n",
    "#         self.N = num_speakers\n",
    "#         self.M = num_utterance\n",
    "\n",
    "#     def build(self, input_shape):\n",
    "#         \"\"\"\n",
    "#         Note:\n",
    "#             Creates the variables of the layer according the input_shape (optional)\n",
    "#         Args:\n",
    "#             input_shape: [#total_utterance, #emb_dim]\n",
    "#         Returns:\n",
    "#         \"\"\"\n",
    "\n",
    "#         super(Centroid_matrix_basic, self).build(input_shape)\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         \"\"\"\n",
    "#         Note:\n",
    "#             Compute centroid matrix of speaker embeddings\n",
    "#         Args:\n",
    "#             inputs: the output of multi lstm and dense layer\n",
    "#         Returns:\n",
    "#             centroid: speaker centroid matrix [#spk, #emb_dim]\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Compute centroids of speaker embeddings\n",
    "#         centroid = tf.keras.backend.reshape(inputs, [self.N, self.M, -1])\n",
    "#         centroid = tf.keras.backend.mean(centroid, axis=1)\n",
    "\n",
    "#         return centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ge2e_loss(Layer):\n",
    "    \"\"\"\n",
    "    Note:\n",
    "        Compute the loss of ge2e in two ways; softmax, contrast\n",
    "    Attributes:\n",
    "        __init__: constructs Ge2e_loss class\n",
    "        call: compute the ge2e loss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_speakers, num_utterance, loss_type=\"contrast\", **kwargs):\n",
    "        \"\"\"\n",
    "        Note:\n",
    "            set up the loss configurations; No. speakers, No. utterances, loss_type\n",
    "        Args:\n",
    "            num_speakers: the number of speakers\n",
    "            num_utterance: the number of utterances\n",
    "            loss_type: \"softmax\" or \"contrast\"\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "\n",
    "        self.N = num_speakers\n",
    "        self.M = num_utterance\n",
    "        self.loss_type = loss_type\n",
    "        super(Ge2e_loss, self).__init__(**kwargs)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'Nspeakers':self.N, 'Mutterance':self.M, 'loss_type': self.loss_type}\n",
    "        base_config = super(Ge2e_loss, self).get_config()\n",
    "        return dict(list(base_config.items())+list(config.items()))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Note:\n",
    "            compute the ge2e loss of a batch\n",
    "        Args:\n",
    "            inputs: the similarities between batch utterances & batch centroids\n",
    "                    [32*4, 32]\n",
    "        Returns:\n",
    "            loss: ge2e loss\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        # contrast loss : - positive + max(negatives)\n",
    "        # loss_positive = tf.math.sigmoid(loss_positive)\n",
    "#         self_block = tf.keras.backend.zeros(shape=[self.M, 1], dtype=tf.float32) # [M,1]\n",
    "#         neg_blocks = tf.keras.backend.ones(shape=[self.M*(self.N-1), 1], dtype=tf.float32) #[M*(N-1),1]\n",
    "\n",
    "#         mask_per_spkr = tf.keras.backend.concatenate([self_block, neg_blocks], axis=0) \n",
    "#         # [M*N,1]\n",
    "#         mask_per_spkr_list = [tf.roll(mask_per_spkr, axis=1, shift=spk_idx*self.M) for spk_idx in range(self.N)]\n",
    "#         neg_mask = tf.keras.backend.concatenate(mask_per_spkr_list, axis=1)\n",
    "#         #[M*N,N]\n",
    "#         pos_mask = tf.Variable(tf.Variable(tf.zeros([self.M*self.N, self.N])))\n",
    "#         comparison = tf.equal( neg_mask, tf.constant(0, dtype=tf.float32) )\n",
    "#         pos_mask = pos_mask.assign( tf.where (comparison, tf.zeros_like(neg_mask), neg_mask))\n",
    "\n",
    "#         loss_positive = tf.multiply(tf.ones_like(pos_mask),pos_mask) - tf.math.sigmoid(pos_mask * inputs)\n",
    "#         loss_negative = tf.keras.backend.max(tf.math.sigmoid(neg_mask * inputs), axis=1, keepdims=True)\n",
    "#         #[tot_utt, 1]\n",
    "#         loss = tf.keras.backend.sum(loss_positive, keepdims=True) + tf.keras.backend.sum(loss_negative, keepdims=True)\n",
    "#         print(loss)\n",
    "        S_correct = tf.concat([inputs[i*self.M:(i+1)*self.M, i:(i+1)] for i in range(self.N)], axis=0)  # colored entries in Fig.1\n",
    "\n",
    "        S_sig = tf.sigmoid(inputs)\n",
    "        S_sig = tf.concat([tf.concat([0*S_sig[i*self.M:(i+1)*self.M, j:(j+1)] if i==j\n",
    "                          else S_sig[i*self.M:(i+1)*self.M, j:(j+1)] for j in range(self.N)], axis=1)\n",
    "                         for i in range(self.N)], axis=0)\n",
    "        loss = tf.reduce_sum(1-tf.sigmoid(S_correct)+tf.reduce_max(S_sig, axis=1, keepdims=True))\n",
    "        return tf.reshape(loss,(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_similarity(enroll, evaluation):\n",
    "    \"\"\"\n",
    "        Calculate similarity matrix\n",
    "        input: enroll (NM * P)\n",
    "               eval (K * P)\n",
    "        return: similarity matrix in tf type (N * K)\n",
    "    \"\"\"\n",
    "#     tmp1, tmp2 = enroll.shape, evaluation.shape\n",
    "#     if tmp1[1]==tmp2[1]: similarity_matrix = np.matmul(evaluation, enroll.T)\n",
    "#     else: similarity_matrix = np.matmul(evaluation, enroll)\n",
    "    similarity_matrix = np.matmul(evaluation, enroll.T)\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EER_estimate(similarity_matrix, S_ground, draw=False):\n",
    "    \"\"\"\n",
    "        estimate EER and draw (Optional)\n",
    "        Input: similarity_matrix, ground, N, M\n",
    "        Output: EER value, fig\n",
    "    \"\"\"\n",
    "    S = similarity_matrix\n",
    "    M = similarity_matrix.shape[0]\n",
    "    N = similarity_matrix.shape[1]\n",
    "    print(similarity_matrix.shape)\n",
    "    print(S_ground.shape)\n",
    "    # initial variable\n",
    "    diff, EER, EER_thres, EER_FAR, EER_FRR = 1.0, 0.0, 0.0, 0.0, 0.0\n",
    "    if draw: all_FAR, all_FRR = [], []\n",
    "\n",
    "    # through threshold to calculate FR and FA rate\n",
    "    for thres in [0.01*i for i in range(100)]:\n",
    "        S_thres = S>thres\n",
    "        TP, TN, FP, FN = 0, 0, 0, 0\n",
    "        \n",
    "        for k in range(M):\n",
    "            for j in range(N):\n",
    "                if S_ground[k,j]:\n",
    "                    if S_thres[k,j]: TP+=1\n",
    "                    else: FN+=1\n",
    "                else:\n",
    "                    if S_thres[k,j]: FP+=1\n",
    "                    else: TN+=1\n",
    "                    \n",
    "        FAR = FP/(FP+TN)\n",
    "        FRR = FN/(TP+FN)\n",
    "        if draw:\n",
    "            all_FAR.append(FAR)\n",
    "            all_FRR.append(FRR)\n",
    "        \n",
    "        # update EER\n",
    "        if diff>np.abs(FAR-FRR):\n",
    "            diff = np.abs(FAR-FRR)\n",
    "            EER = (FAR+FRR)/2\n",
    "            EER_thres, EER_FAR, EER_FRR = thres, FAR, FRR\n",
    "            print('confusion matrix')\n",
    "            print('%5d, %5d, %5d, %5d'%(TP,FN,TN,FP))\n",
    "    print(\"EER: %0.2f (thres: %0.2f, FAR: %0.2f, FRR: %0.2f) \\n\"%(EER*100.0, EER_thres*1.0, EER_FAR*100.0, EER_FRR*100.0))\n",
    "    return EER\n",
    "    \n",
    "    \n",
    "    if draw:\n",
    "        thres = np.linspace(0,1,100)\n",
    "        plt.plot(thres, all_FRR)\n",
    "        plt.plot(thres, all_FAR)\n",
    "        plt.legend(['FRR','FAR'])\n",
    "        plt.title('keyword, EER=%1.2f%%'%(EER*100))\n",
    "        plt.xlabel('threshold')\n",
    "        plt.ylabel('Error rate')\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
